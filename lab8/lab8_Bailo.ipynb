{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Task 1"
      ],
      "metadata": {
        "id": "qRx182VY5SVC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAI7pj_l5P-_",
        "outputId": "91516c78-4ebe-4cc2-c332-3d83681177c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 100: loss=3.15241456, k=2.0988, b=0.6194\n",
            "Step 200: loss=4.06156540, k=2.0327, b=0.7751\n",
            "Step 300: loss=4.08008003, k=2.0160, b=0.8644\n",
            "Step 400: loss=4.14385509, k=2.0287, b=0.9507\n",
            "Step 500: loss=3.83299232, k=2.0216, b=1.0277\n",
            "Step 600: loss=4.36223555, k=2.0278, b=1.0653\n",
            "Step 700: loss=4.71020460, k=1.9691, b=1.0714\n",
            "Step 800: loss=4.70024729, k=1.9879, b=1.0585\n",
            "Step 900: loss=3.50888586, k=1.9836, b=1.0756\n",
            "Step 1000: loss=3.51240206, k=2.0476, b=1.0881\n",
            "Step 1100: loss=3.55369425, k=1.9962, b=1.0825\n",
            "Step 1200: loss=4.40443039, k=2.0293, b=1.1286\n",
            "Step 1300: loss=3.88142848, k=2.0023, b=1.1235\n",
            "Step 1400: loss=4.19310713, k=1.9883, b=1.1236\n",
            "Step 1500: loss=3.72393990, k=2.0300, b=1.1159\n",
            "Step 1600: loss=4.06284285, k=1.9598, b=1.0984\n",
            "Step 1700: loss=3.63576937, k=1.9665, b=1.1024\n",
            "Step 1800: loss=4.53438616, k=2.0004, b=1.1123\n",
            "Step 1900: loss=4.42007971, k=1.9991, b=1.0977\n",
            "Step 2000: loss=4.05266523, k=1.9670, b=1.0796\n",
            "Step 2100: loss=4.32937145, k=1.9398, b=1.0792\n",
            "Step 2200: loss=3.76376653, k=2.0562, b=1.0997\n",
            "Step 2300: loss=3.24066019, k=1.9676, b=1.1042\n",
            "Step 2400: loss=4.07774019, k=2.0060, b=1.0887\n",
            "Step 2500: loss=4.02605629, k=1.9423, b=1.0938\n",
            "Step 2600: loss=3.46992207, k=1.9554, b=1.1321\n",
            "Step 2700: loss=4.16199732, k=1.9679, b=1.1432\n",
            "Step 2800: loss=3.18897867, k=1.9605, b=1.1745\n",
            "Step 2900: loss=4.22026014, k=1.9807, b=1.1620\n",
            "Step 3000: loss=3.35743165, k=1.9527, b=1.1391\n",
            "Step 3100: loss=4.08970070, k=1.9482, b=1.1684\n",
            "Step 3200: loss=5.38902140, k=1.9794, b=1.1756\n",
            "Step 3300: loss=3.77407765, k=1.9738, b=1.1444\n",
            "Step 3400: loss=4.02389860, k=1.9865, b=1.1222\n",
            "Step 3500: loss=4.12160444, k=2.0127, b=1.1454\n",
            "Step 3600: loss=3.49918890, k=1.9652, b=1.1218\n",
            "Step 3700: loss=3.36013794, k=1.9789, b=1.1242\n",
            "Step 3800: loss=3.53386045, k=1.9702, b=1.1241\n",
            "Step 3900: loss=4.84223938, k=1.9841, b=1.1106\n",
            "Step 4000: loss=4.23074150, k=2.0267, b=1.0983\n",
            "Step 4100: loss=4.12004519, k=1.9945, b=1.1140\n",
            "Step 4200: loss=4.22882223, k=1.9844, b=1.0986\n",
            "Step 4300: loss=4.03303051, k=1.9673, b=1.1074\n",
            "Step 4400: loss=3.57518220, k=2.0076, b=1.1051\n",
            "Step 4500: loss=5.11777306, k=2.0043, b=1.0879\n",
            "Step 4600: loss=4.59739733, k=2.0049, b=1.1210\n",
            "Step 4700: loss=4.07819843, k=1.9932, b=1.0989\n",
            "Step 4800: loss=3.87612033, k=1.9673, b=1.1246\n",
            "Step 4900: loss=3.57076955, k=1.9653, b=1.1093\n",
            "Step 5000: loss=4.19285297, k=1.9164, b=1.1207\n",
            "Step 5100: loss=3.23616028, k=1.9692, b=1.1392\n",
            "Step 5200: loss=4.63133621, k=1.9404, b=1.1402\n",
            "Step 5300: loss=3.20588136, k=1.9742, b=1.1364\n",
            "Step 5400: loss=3.30960941, k=1.9929, b=1.1298\n",
            "Step 5500: loss=3.44961023, k=1.9679, b=1.1167\n",
            "Step 5600: loss=3.68998170, k=1.9479, b=1.1209\n",
            "Step 5700: loss=4.02963400, k=1.9882, b=1.1413\n",
            "Step 5800: loss=4.21067095, k=1.9949, b=1.1433\n",
            "Step 5900: loss=4.87548971, k=1.9942, b=1.1237\n",
            "Step 6000: loss=4.02446032, k=1.9915, b=1.1081\n",
            "Step 6100: loss=4.30354261, k=1.9684, b=1.0902\n",
            "Step 6200: loss=4.53893089, k=1.9781, b=1.1002\n",
            "Step 6300: loss=4.30413342, k=1.9569, b=1.0912\n",
            "Step 6400: loss=4.13669252, k=2.0019, b=1.1097\n",
            "Step 6500: loss=4.04068899, k=1.9940, b=1.1313\n",
            "Step 6600: loss=3.92206788, k=1.9796, b=1.1253\n",
            "Step 6700: loss=4.05630207, k=1.9704, b=1.1087\n",
            "Step 6800: loss=4.17189360, k=1.9894, b=1.1091\n",
            "Step 6900: loss=3.56992269, k=1.9881, b=1.1115\n",
            "Step 7000: loss=3.45707488, k=2.0087, b=1.1209\n",
            "Step 7100: loss=3.29596281, k=1.9763, b=1.1392\n",
            "Step 7200: loss=4.12857676, k=1.9929, b=1.1298\n",
            "Step 7300: loss=3.47775173, k=1.9574, b=1.1555\n",
            "Step 7400: loss=4.22071075, k=1.9526, b=1.1757\n",
            "Step 7500: loss=3.84684610, k=1.9770, b=1.1626\n",
            "Step 7600: loss=4.26581955, k=1.9672, b=1.1426\n",
            "Step 7700: loss=4.51941061, k=2.0198, b=1.1571\n",
            "Step 7800: loss=3.95858836, k=1.9704, b=1.1482\n",
            "Step 7900: loss=3.61499810, k=1.9771, b=1.1605\n",
            "Step 8000: loss=4.12892103, k=1.9525, b=1.1466\n",
            "Step 8100: loss=4.12568569, k=1.9880, b=1.1385\n",
            "Step 8200: loss=3.78673315, k=1.9766, b=1.1235\n",
            "Step 8300: loss=4.28314924, k=1.9636, b=1.1251\n",
            "Step 8400: loss=3.64593315, k=2.0022, b=1.1515\n",
            "Step 8500: loss=3.24615407, k=1.9971, b=1.1179\n",
            "Step 8600: loss=3.39489722, k=2.0070, b=1.1211\n",
            "Step 8700: loss=3.73935795, k=2.0116, b=1.1236\n",
            "Step 8800: loss=3.47870564, k=1.9984, b=1.1318\n",
            "Step 8900: loss=3.83079219, k=1.9532, b=1.1342\n",
            "Step 9000: loss=3.61023188, k=2.0165, b=1.1233\n",
            "Step 9100: loss=3.73818207, k=1.9961, b=1.1297\n",
            "Step 9200: loss=4.71313334, k=1.9875, b=1.1328\n",
            "Step 9300: loss=4.24549484, k=1.9541, b=1.1095\n",
            "Step 9400: loss=4.40143299, k=2.0104, b=1.1178\n",
            "Step 9500: loss=3.67868948, k=2.0220, b=1.1150\n",
            "Step 9600: loss=3.78784251, k=2.0199, b=1.1205\n",
            "Step 9700: loss=4.09291363, k=1.9942, b=1.1214\n",
            "Step 9800: loss=4.10751057, k=1.9733, b=1.1171\n",
            "Step 9900: loss=4.51879597, k=1.9990, b=1.1180\n",
            "Step 10000: loss=3.52887583, k=2.0039, b=1.1125\n",
            "Step 10100: loss=3.85759592, k=1.9291, b=1.0972\n",
            "Step 10200: loss=3.57041526, k=1.9892, b=1.0934\n",
            "Step 10300: loss=3.18752265, k=1.9898, b=1.0823\n",
            "Step 10400: loss=2.81418705, k=1.9752, b=1.0876\n",
            "Step 10500: loss=4.46691084, k=2.0174, b=1.0823\n",
            "Step 10600: loss=3.72849250, k=2.0064, b=1.0913\n",
            "Step 10700: loss=3.66333914, k=1.9787, b=1.0713\n",
            "Step 10800: loss=4.45760632, k=1.9812, b=1.0843\n",
            "Step 10900: loss=3.62798786, k=1.9288, b=1.1061\n",
            "Step 11000: loss=3.66335869, k=2.0008, b=1.0936\n",
            "Step 11100: loss=3.56377172, k=2.0105, b=1.1110\n",
            "Step 11200: loss=4.37616110, k=1.8988, b=1.0842\n",
            "Step 11300: loss=3.48266411, k=1.9559, b=1.0895\n",
            "Step 11400: loss=2.96472692, k=1.9865, b=1.0939\n",
            "Step 11500: loss=3.34578180, k=1.9695, b=1.1171\n",
            "Step 11600: loss=3.41896415, k=1.9856, b=1.1236\n",
            "Step 11700: loss=3.15923333, k=1.9730, b=1.1086\n",
            "Step 11800: loss=4.08520746, k=2.0105, b=1.1172\n",
            "Step 11900: loss=3.95333409, k=2.0093, b=1.1173\n",
            "Step 12000: loss=4.15832615, k=1.9863, b=1.1013\n",
            "Step 12100: loss=3.12081981, k=1.9973, b=1.1032\n",
            "Step 12200: loss=3.70537782, k=1.9861, b=1.1110\n",
            "Step 12300: loss=3.96499515, k=2.0174, b=1.1011\n",
            "Step 12400: loss=4.14829206, k=2.0217, b=1.1155\n",
            "Step 12500: loss=2.59687495, k=2.0082, b=1.1211\n",
            "Step 12600: loss=3.40025401, k=1.9980, b=1.1225\n",
            "Step 12700: loss=3.89800167, k=2.0260, b=1.1288\n",
            "Step 12800: loss=3.98096442, k=1.9862, b=1.1242\n",
            "Step 12900: loss=4.72046947, k=1.9633, b=1.1237\n",
            "Step 13000: loss=4.46852684, k=2.0023, b=1.1381\n",
            "Step 13100: loss=4.36338091, k=1.9470, b=1.0967\n",
            "Step 13200: loss=3.18458104, k=1.9574, b=1.0851\n",
            "Step 13300: loss=3.42389274, k=1.9904, b=1.1103\n",
            "Step 13400: loss=3.98326731, k=1.9571, b=1.1067\n",
            "Step 13500: loss=4.34439945, k=1.9600, b=1.0997\n",
            "Step 13600: loss=4.44958973, k=1.9574, b=1.0933\n",
            "Step 13700: loss=3.21521401, k=1.9816, b=1.1009\n",
            "Step 13800: loss=5.12677670, k=1.9847, b=1.0707\n",
            "Step 13900: loss=3.73789787, k=2.0067, b=1.0890\n",
            "Step 14000: loss=3.97852111, k=2.0035, b=1.0763\n",
            "Step 14100: loss=3.82630253, k=1.9778, b=1.0578\n",
            "Step 14200: loss=4.90873861, k=2.0094, b=1.0896\n",
            "Step 14300: loss=3.89196420, k=2.0409, b=1.1043\n",
            "Step 14400: loss=4.13141203, k=2.0149, b=1.0886\n",
            "Step 14500: loss=3.54584599, k=2.0046, b=1.0983\n",
            "Step 14600: loss=4.46018028, k=1.9682, b=1.1003\n",
            "Step 14700: loss=4.43805742, k=1.9695, b=1.1114\n",
            "Step 14800: loss=3.26560760, k=1.9505, b=1.1229\n",
            "Step 14900: loss=4.36638069, k=1.9305, b=1.1336\n",
            "Step 15000: loss=4.27592993, k=2.0048, b=1.1436\n",
            "Step 15100: loss=3.79264164, k=1.9253, b=1.1109\n",
            "Step 15200: loss=2.74423909, k=1.9982, b=1.1216\n",
            "Step 15300: loss=3.79566002, k=2.0099, b=1.1057\n",
            "Step 15400: loss=4.23738241, k=2.0255, b=1.1375\n",
            "Step 15500: loss=3.81906104, k=2.0153, b=1.1138\n",
            "Step 15600: loss=3.71797323, k=2.0003, b=1.1379\n",
            "Step 15700: loss=4.69628239, k=1.9946, b=1.1406\n",
            "Step 15800: loss=4.43877602, k=1.9914, b=1.1267\n",
            "Step 15900: loss=4.27824306, k=1.9595, b=1.1158\n",
            "Step 16000: loss=3.69098759, k=1.9710, b=1.1427\n",
            "Step 16100: loss=4.48568296, k=1.9769, b=1.1506\n",
            "Step 16200: loss=3.94460058, k=2.0227, b=1.1445\n",
            "Step 16300: loss=4.33645487, k=1.9948, b=1.1528\n",
            "Step 16400: loss=3.71455956, k=1.9914, b=1.1382\n",
            "Step 16500: loss=3.83349729, k=1.9904, b=1.1298\n",
            "Step 16600: loss=3.49686575, k=1.9707, b=1.0990\n",
            "Step 16700: loss=3.96945286, k=2.0010, b=1.0786\n",
            "Step 16800: loss=3.37603402, k=1.9902, b=1.0659\n",
            "Step 16900: loss=3.38851070, k=1.9771, b=1.0789\n",
            "Step 17000: loss=6.05709362, k=1.9768, b=1.0716\n",
            "Step 17100: loss=3.91439176, k=1.9786, b=1.0556\n",
            "Step 17200: loss=3.57571745, k=2.0183, b=1.0907\n",
            "Step 17300: loss=4.36574411, k=1.9625, b=1.0801\n",
            "Step 17400: loss=5.02062130, k=1.9960, b=1.1136\n",
            "Step 17500: loss=3.79835844, k=2.0052, b=1.0992\n",
            "Step 17600: loss=4.33441782, k=1.9803, b=1.1061\n",
            "Step 17700: loss=3.40256143, k=1.9896, b=1.0847\n",
            "Step 17800: loss=3.93337893, k=2.0029, b=1.0875\n",
            "Step 17900: loss=2.88591480, k=2.0002, b=1.0939\n",
            "Step 18000: loss=4.33071852, k=2.0215, b=1.1253\n",
            "Step 18100: loss=3.47222233, k=1.9914, b=1.1201\n",
            "Step 18200: loss=4.09446478, k=1.9709, b=1.1010\n",
            "Step 18300: loss=3.81930327, k=2.0099, b=1.1153\n",
            "Step 18400: loss=4.23922157, k=2.0051, b=1.0886\n",
            "Step 18500: loss=4.18212461, k=1.9821, b=1.0996\n",
            "Step 18600: loss=3.26970148, k=2.0022, b=1.0846\n",
            "Step 18700: loss=3.72266459, k=1.9792, b=1.0963\n",
            "Step 18800: loss=3.93216515, k=1.9840, b=1.0961\n",
            "Step 18900: loss=3.84095860, k=2.0084, b=1.1064\n",
            "Step 19000: loss=3.79307914, k=1.9638, b=1.1222\n",
            "Step 19100: loss=3.87968540, k=1.9485, b=1.1021\n",
            "Step 19200: loss=3.82824826, k=2.0155, b=1.1211\n",
            "Step 19300: loss=4.72091341, k=1.9272, b=1.0916\n",
            "Step 19400: loss=5.31267834, k=1.9842, b=1.1032\n",
            "Step 19500: loss=4.51564503, k=2.0100, b=1.1291\n",
            "Step 19600: loss=4.84159660, k=2.0132, b=1.1523\n",
            "Step 19700: loss=3.94482875, k=2.0030, b=1.1299\n",
            "Step 19800: loss=2.87955141, k=1.9820, b=1.1271\n",
            "Step 19900: loss=2.91919589, k=1.9638, b=1.1333\n",
            "Step 20000: loss=2.52396631, k=1.9937, b=1.1441\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "# Генеруємо дані\n",
        "n_samples, batch_size, num_steps = 1000, 100, 20000\n",
        "X_data = np.random.uniform(1, 10, (n_samples, 1))\n",
        "y_data = 2 * X_data + 1 + np.random.normal(0, 2, (n_samples, 1))\n",
        "\n",
        "# Визначаємо плейсхолдери\n",
        "X = tf.placeholder(tf.float32, shape=(batch_size, 1), name=\"X\")\n",
        "y = tf.placeholder(tf.float32, shape=(batch_size, 1), name=\"y\")\n",
        "\n",
        "# Модель лінійної регресії\n",
        "with tf.variable_scope('linear_regression'):\n",
        "    k = tf.Variable(tf.random.normal((1, 1)), name='slope')\n",
        "    b = tf.Variable(tf.zeros((1,)), name='bias')\n",
        "    y_pred = tf.matmul(X, k) + b\n",
        "\n",
        "# Функція втрат\n",
        "loss = tf.reduce_mean(tf.square(y - y_pred))\n",
        "\n",
        "# Оптимізатор\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss)\n",
        "\n",
        "# Параметри відображення\n",
        "display_step = 100\n",
        "\n",
        "# Тренувальна сесія\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for i in range(num_steps):\n",
        "        # Випадковий вибір міні-батчів\n",
        "        indices = np.random.choice(n_samples, batch_size)\n",
        "        X_batch, y_batch = X_data[indices], y_data[indices]\n",
        "\n",
        "        # Запуск оптимізації та отримання значення втрат\n",
        "        _, loss_val, k_val, b_val = sess.run([optimizer, loss, k, b], feed_dict={X: X_batch, y: y_batch})\n",
        "\n",
        "        # Виведення результатів\n",
        "        if (i + 1) % display_step == 0:\n",
        "            print(f'Step {i + 1}: loss={loss_val:.8f}, k={k_val[0][0]:.4f}, b={b_val[0]:.4f}')"
      ]
    }
  ]
}